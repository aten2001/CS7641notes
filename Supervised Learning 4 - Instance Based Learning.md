# Supervise Learning 4 - Instance Base Learning

## Introduction

Up until this point we look at <x_1, y_1>, ..., <x_n, y_n> to produce a f(x) = w*x + y.

Instance based learning says store <x_1, y_1>, ..., <x_n, y_n> in a database and so f(x) = lookup(x)

There are several quirks:
- It remembers - if you query something that you trained on you get the value rather than a rounded or smoothed out value
- Fast "learning"
- It's simple.
- Seems conservative - no generalization?
- Memorization - overfitting is a big problem; sensitive to noise
- What happens if the same x shows up with multiple y values?

This version might be a little too literal

## KNN



## Doman Knowledge



## KNN Bias



## Curse of Dimensionalaity



## Summary

