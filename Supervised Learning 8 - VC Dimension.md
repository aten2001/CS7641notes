# Supervised Learning 8 - VC Dimension

## Infinite hypothesis space

last time we found that:

m >= 1 / \epsilon (ln|H| + ln(1 / \delta))

The problems is if you have an infinite set of hypothesis this equation is infinite. This is bad because linear separators, artificial neural networks, and decision trees with continuous inputs all have infinite hypothesis spaces. Of the various algorithms discussed so far, pretty much only decision trees with discrete inputs don't!

## Maybe it's not so bad



## Power of a hypothesis space



## What does VC stand for?



## Internal training



## Linear separators



## The ring



## Polygons



## Sample complexity



## VC of finite H



## Summary


